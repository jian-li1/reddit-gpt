{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c447d60-1b4e-43c4-a16f-c1c73f59c776",
   "metadata": {},
   "source": [
    "**Relevant resources:**\n",
    "\n",
    "* [Unsloth Github](https://github.com/unslothai/unsloth)\n",
    "* [Llama 3.1 Base 8b Model](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit)\n",
    "* [Llama 3.1 Instruct 8b Model](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit)\n",
    "* [Llama-3.1 8b + Unsloth 2x faster finetuning](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)\n",
    "* [Llama-3 8b Instruct Unsloth 2x faster finetuning](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a2827-bf0d-4de3-873b-7cab3c2b2123",
   "metadata": {},
   "source": [
    "**Load model from Unsloth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e7018-0b5d-41db-89b7-d6dcaf4a69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import gc\n",
    "import os \n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec70efb-0a7d-4a6c-be9e-4d3377e0dbc7",
   "metadata": {},
   "source": [
    "**Add LoRA adapters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac577da5-300f-406a-bc38-80c9bd51f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e647f2-a513-4bf2-b4f9-667e439bb2c2",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e492c36-e004-4440-ab7a-8926ad0edd84",
   "metadata": {},
   "source": [
    "* [Loading dataset from different formats](https://huggingface.co/docs/datasets/en/loading)\n",
    "* [Loading dataset from SQLite3 database](https://huggingface.co/docs/datasets/main/en/tabular_load#sqlite)\n",
    "* [Llama 3.1 Instruct Prompt Format](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/#llama-3.1-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b5b14-34c8-4980-bcac-fd01b90ec39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "instructions = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\"\"\"\n",
    "\n",
    "def format_prompts(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for input_txt, output_txt in zip(inputs, outputs):\n",
    "        text = [\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": input_txt},\n",
    "            {\"role\": \"assistant\", \"content\": output_txt}\n",
    "        ]\n",
    "        texts.append(tokenizer.apply_chat_template(text, tokenize=False))\n",
    "    return {\"text\": texts,}\n",
    "\n",
    "ds_file_path = \"/mnt/d/Code/reddit/dataset/combined.json\"\n",
    "dataset = load_dataset(\"json\", data_files=ds_file_path, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.map(format_prompts, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72d5b1-54af-4d08-9611-f40d1ab5541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14dafa8-373c-4787-9560-a80f41ff3b67",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99adb7ef-badd-47e3-865e-ab7b06b2aa1b",
   "metadata": {},
   "source": [
    "* [Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b41f5-0b80-43c0-b156-8dcc7b494163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    # train_dataset = train_dataset,\n",
    "    # eval_dataset = eval_dataset, # Set this if using evaluation dataset\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 6,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 50,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        # max_steps = 150,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        save_steps = 250,\n",
    "        save_total_limit = 8,\n",
    "        # eval_strategy = \"steps\", # Set this if using evaluation dataset\n",
    "        # eval_steps = 100, # Set this if using evaluation dataset\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a4c12-a9a8-47f2-a821-9ea9ac377bac",
   "metadata": {},
   "source": [
    "**Show current memory stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c81ddd-0e1f-463b-a4c2-955f998ab87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d760f5e0",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3f2f2-c9d1-40a6-930c-7416e77e128d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()\n",
    "# trainer_stats = trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1124c41-fe92-4e64-ba07-b55098ca60b6",
   "metadata": {},
   "source": [
    "**Show final memory and time stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9fe79-4aa1-40ba-a227-5bd63ca1a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d32871-7d4e-407e-9e36-70314731fb4d",
   "metadata": {},
   "source": [
    "**Show training loss over time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d79ca7-36e1-403b-9493-90e6a1a315f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the loss values\n",
    "loss_values = trainer.state.log_history\n",
    "\n",
    "# Extracting only the loss from each log entry\n",
    "training_loss = [entry['loss'] for entry in loss_values if 'loss' in entry]\n",
    "\n",
    "# Plotting the loss over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train/Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc8de40-43f8-4b8d-ae86-523b4680ed5f",
   "metadata": {},
   "source": [
    "**Save finetuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93119755-1659-450b-aebb-34bcab78b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"model\")\n",
    "trainer.save_model(\"model\")\n",
    "trainer.save_state()\n",
    "# print(\"model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6987f811-f09f-4ac2-9e16-3b275900338d",
   "metadata": {},
   "source": [
    "### Try inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa43bfe-8e22-458c-9993-7031b231c34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instructions = \"You are a helpful AI assistant.\"\n",
    "\n",
    "prompt = \"\"\n",
    "\n",
    "msgs = [\n",
    "    {\"role\": \"system\", \"content\": instructions},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    msgs,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\"\n",
    ").to(\"cuda\")\n",
    "attention_mask = (inputs != tokenizer.pad_token_id).int()\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids = inputs, \n",
    "    attention_mask = attention_mask, \n",
    "    min_new_tokens = 64, \n",
    "    max_new_tokens = 1024,\n",
    "    temperature = 0.8,\n",
    "    repetition_penalty=1.25,\n",
    "    top_p = 0.9,\n",
    "    use_cache = True\n",
    ")\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a5e9c-0226-4105-9439-71ac0cd4e929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
